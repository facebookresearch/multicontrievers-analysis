{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_feature_encoding\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SENTENCES = ['I love mom\\'s cooking',\n",
    "                  'I love how you never reply back..',\n",
    "                  'I love cruising with my homies',\n",
    "                  'I love messing with yo mind!!',\n",
    "                  'I love you and now you\\'re just gone..',\n",
    "                  'This is shit',\n",
    "                  'This is the shit']\n",
    "\n",
    "maxlen = 30\n",
    "batch_size = 32\n",
    "\n",
    "print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "st = SentenceTokenizer(vocabulary, maxlen)\n",
    "tokenized, _, _ = st.tokenize_sentences(TEST_SENTENCES)\n",
    "\n",
    "print('Loading model from {}.'.format(PRETRAINED_PATH))\n",
    "model = torchmoji_feature_encoding(PRETRAINED_PATH)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print TEST_SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Encoding texts..')\n",
    "encoding = model(tokenized)\n",
    "\n",
    "print('First 5 dimensions for sentence: {}'.format(TEST_SENTENCES[0]))\n",
    "print encoding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_race_dir = '../data/processed/sent_race/'\n",
    "sent_gender_dir = '../data/processed/sent_gender/'\n",
    "mention_age_dir = '../data/processed/author_mention_age/'\n",
    "mention_gender_dir = '../data/processed/author_mention_gender/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(d):\n",
    "    with open(d + 'vocab', 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "        vocab = map(lambda s: s.strip(), vocab)\n",
    "    def to_words(sen):\n",
    "        s = []\n",
    "        for w in sen:\n",
    "            s.append(vocab[w])\n",
    "        return s\n",
    "    \n",
    "    with open(d + 'pos_pos', 'r') as f:\n",
    "        pos_pos = f.readlines()\n",
    "        pos_pos = [map(int, sen.split(' ')) for sen in pos_pos]\n",
    "        pos_pos = pos_pos[:total]\n",
    "        pos_pos = map(to_words, pos_pos)\n",
    "    with open(d + 'pos_neg', 'r') as f:\n",
    "        pos_neg = f.readlines()\n",
    "        pos_neg = [map(int, sen.split(' ')) for sen in pos_neg]\n",
    "        pos_neg = pos_neg[:total]\n",
    "        pos_neg = map(to_words, pos_neg)\n",
    "    with open(d + 'neg_pos', 'r') as f:\n",
    "        neg_pos = f.readlines()\n",
    "        neg_pos = [map(int, sen.split(' ')) for sen in neg_pos]\n",
    "        neg_pos = neg_pos[:total]\n",
    "        neg_pos = map(to_words, neg_pos)\n",
    "    with open(d + 'neg_neg', 'r') as f:\n",
    "        neg_neg = f.readlines()\n",
    "        neg_neg = [map(int, sen.split(' ')) for sen in neg_neg]\n",
    "        neg_neg = neg_neg[:total]\n",
    "        neg_neg = map(to_words, neg_neg)\n",
    "    \n",
    "    return pos_pos, pos_neg, neg_pos, neg_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 150\n",
    "batch_size = 32\n",
    "\n",
    "print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "st = SentenceTokenizer(vocabulary, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_join(sents):\n",
    "    a = []\n",
    "    for s in sents:\n",
    "        try:\n",
    "            a.append(' '.join([x.decode('utf-8') for x in s]))\n",
    "        except:\n",
    "            print s\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pos, pos_neg, neg_pos, neg_neg = get_sentences(sent_race_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pos = sent_join(pos_pos)\n",
    "pos_neg = sent_join(pos_neg)\n",
    "neg_pos = sent_join(neg_pos)\n",
    "neg_neg = sent_join(neg_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../data/orig_sent_race'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode(in_data, bs_size):\n",
    "    encoded_data = []\n",
    "    for i in tqdm(range(0, len(in_data), bs_size)):\n",
    "        tokenized, _, _ = st.tokenize_sentences(in_data[i: i + bs_size])\n",
    "        encoded_batch = model(tokenized)\n",
    "        encoded_data.extend(encoded_batch)\n",
    "    return np.array(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pos[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = batch_encode(pos_pos[:5000], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, name in zip([pos_pos, pos_neg, neg_neg, neg_pos], ['pos_pos', 'pos_neg', 'neg_neg', 'neg_pos']):\n",
    "    encoded_data = batch_encode(d, bs_size=1000)\n",
    "    np.save(out_dir + '/{}.npy'.format(name), encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(data, file_name):\n",
    "    import io\n",
    "    encodable_data = []\n",
    "    with io.open(file_name, \"w\", encoding=\"utf-8\") as my_file:\n",
    "        for line in data:\n",
    "            try:\n",
    "                my_file.write(line.encode('utf-8') + '\\n')\n",
    "                encodable_data.append(line)\n",
    "            except:\n",
    "                pass\n",
    "    return encodable_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pos_pos = save_file(pos_pos, out_dir + '/pos_pos.txt')\n",
    "new_pos_neg = save_file(pos_neg, out_dir + '/pos_neg.txt')\n",
    "new_neg_pos = save_file(neg_pos, out_dir + '/neg_pos.txt')\n",
    "new_neg_neg = save_file(neg_neg, out_dir + '/neg_neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, name in zip([new_pos_pos, new_pos_neg, new_neg_neg, new_neg_pos],\n",
    "                   ['pos_pos', 'pos_neg', 'neg_neg', 'neg_pos']):\n",
    "    encoded_data = batch_encode(d, bs_size=1000)\n",
    "    np.save(out_dir + '/{}.npy'.format(name), encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoji",
   "language": "python",
   "name": "emoji"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
